# goit-web-hw-09

* Виберіть бібліотеку BeautifulSoup або фреймворк Scrapy.
* Ви повинні виконати скрапінг сайту http://quotes.toscrape.com. 
* Ваша мета отримати два файли: 
  * qoutes.json, куди помістіть всю інформацію про цитати, з усіх сторінок сайту 
  * authors.json, де буде знаходитись інформація про авторів зазначених цитат. 
* Структура файлів json повинна повністю збігатися з попереднього домашнього завдання. 
* Виконайте раніше написані скрипти для завантаження json файлів у хмарну базу даних для отриманих файлів
* Попередня домашня робота повинна коректно працювати з новою отриманою базою даних.

# Додаткове завдання
* Використовуйте для скрапінгу фреймворк Scrapy. 
* Запуск краулера повинен бути виконаний у вигляді єдиного скрипта main.py

----
## Примітки
1. налаштувати проект під Scrapy
    ```
    pip install Scrapy
    ```
2. Перейти в папку де буде створений проект і виконати команду ініціалізації
    ```
    scrapy startproject <project name>
    ```
    
3. Після створення проекту потрібно перейти в нову папку та написати наступну команду - Команда створить шаблон павука в spyders/<spider name>.py
    ```
    scrapy genspider <spider name> <domain>
    scrapy genspider quotes quotes.toscrape.com
    ```
4. Написати обробний для спайдеру 
   * <spider name>.py - описати основний алгоритм проходження по сторінці для збору інфи
   * pipelines.py - описати інтерфейс збереження в json-файли
   * items.py - описати модель даних кожного з елементів

5. запуск павука з консолі: 
   ```
   scrapy crawl <spider name>
   scrapy crawl quotes
    ```
   *Команду треба запускати з тієї ж папки, де знаходиться scrapy.cfg, тобто з кореня проєкту*